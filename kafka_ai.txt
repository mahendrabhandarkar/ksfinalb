----------------------------------------------------------------------------------

Nearest Neighbour --  Vector 2 D parsor --- HashValue --- 

2D Object -- similarity search
stostasthic --- Similarity search

Java ML Library for Model Creation. Tensorflow used to create tensors, etc.

Tightly Coupled Architecture

 Kafka refers source as events [ex: Inventory, Order, Payment], Topics [Characterized event - identification of event] --
 Kafka --- Producer, Kafka, Consumer
 
 PubSub Model, Single Sendor Model -- 
 Kafka relies on another servce named ZooKeeper -- a distrbuted cordination system - to function.
 
 What is ZooKeeper --
 -- Maintaining a register of all active brokers in the cluster.
 Leader Election --
 -- Each kafka partition has a leader broker
 -- ZooKeeper facilities the selection of the leader
 
 Metadata and configuration Management
 
 Kraf concepts --- 
 
 Kafka Core Concepts -- Producers --- Consumer [update inventory database].
 
 Organizing and Scaling Orders
 
 Partitions --- Divide topics into sections.
			--- Resemble Kitchen stations
			
 Consumer Groups --- for Scalable Message Consumption
 
 Rabbing MQ follow -- Single Queue Structure [example: TV channels].
 Kafka --- Video on demand (streaming(.
 
 Flink --- Apache Flink --- 
 Kafka- is messaging system --
 Flink -- High capabilities on processing --- Used for transformation -- Used for event streaming processing.
 
 github.com // Kafka - Flink - Demo-Master --- kafka-ui
 
 Apache Flink UI --- run to view on machine
 
 To support python -- adding another library to support in flink.
 
 maheboob.dev --- Developer Kafka
 
--- Passed message to kafka --- Before to store data -----------

Batch Processing -- Stream Processing ---

----------------------------------------------------------------------------------------------------------------------------------------------

 Audit Logs at Confluent
 
 Audit Logs contain information about who tried to do what, when they tried, and whether or not the system gave permission to proceed.
 Sample Audit Log:
 
 Organization Auditable Event Methods on Confluent:
 
 Let's Build a Pipeline -- How to send audit logs E2E from generation to consumption
 
 Decoupled and Asynchronous Operations
 
 Source -- Ths is going to be Variety of microservices across multiple languages and cloud providers. The number of producers would increase over time but the set it still somewhat constant.
 
 Backbone: Apache Kafka -- Acts as a powerful, highly available and durable queue for asynchronous operations: -- Message Reusabilty
 
 Destionation: Apache kafka --- Serves as the primary destination for further processing, or feeding into long-term storage solutions and ideal for contineous processing and integration with other systems.
 
 Multiple Topics -- 
 Problem #1: Metadata
 We don't know where to send it. Producer should not be sending out.
 Solution: bind userid and orgId to mark cluster destination. And we will send it there. Now we need a Router Service that reads the metadata figures out the destination and sends them there.
 
 Aspect Oriented Programming --- 
 --- Every activity of user should be audited.
 --- 
 
 Destination Cluster --- Create topics and tell customer to go and consume it.
 
 Amrapali Case --- Deleted the records and digital forensic agency. 
 
 SIM tool logs can also be send to Kafka for events. 
 
 Problem #2: Missing Metadata Problem.
 You might run into solutions where the metadata generation has failed or is delayed. As a result, while sending the logs you'ill reject as the destination is absent.
 
 Solution: Retries topic [Retry -1, Retry -2, Retry - 3].
 
 Problem #3: Redaction and Internal Logs:
 All the events produced may not be fit for consumption for the user. It may need additional filtering like removing internal users, internal IP addresses and internal audit logs.
 
 Kafka Streams:
 Sample Code
 Horizontally Scalable
 Exactly Once Processing
 
 2.5GB Heap
 
 Logstash -- Send and reads to kafka also. but it is used lots of RAM.
 
 Processor Service --- 
 
 Problem #4: How to you make sure each service is sending Audit Logs?
 Solution:
 
 Kraft -- Broker
 
 Health Check Audit Logs
 
 - The Microservice need to continuously 
 
 Maintaining the Pipeline: Metrics and Monitoring
 
 --- Source Metrics -- Message Size
 --- Processing Merics -- Events dropped per second
 --- Router Metrics -- E2E Latency
 
 https://developer.confluent.io/

-----------------------------------------------------------------------------------------------------------------------------------------------

Create your own AI model similar to llama3.2 using java?

To create your own AI model similar to Llama 3.2 using Java, you need to understand both the architecture and the practical considerations of building large language models (LLMs) like Llama 3.2. Here’s a concise breakdown:

What Makes Llama 3.2 Special?
Architecture: Llama 3.2 is an auto-regressive language model built on an optimized transformer architecture. It supports multilingual text and, in some versions, vision (image) input. The model sizes range from 1B to 90B parameters, but the lightweight 1B and 3B models are designed for edge and mobile devices.

Training Techniques: Instruction-tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) for alignment with human preferences.

Applications: Text summarization, rewriting, personal assistants, multilingual dialogue, and (in vision models) image understanding.

Building a Llama 3.2-like Model in Java: Steps and Considerations
1. Model Architecture
Transformer Implementation: Llama 3.2 uses a transformer architecture, specifically optimized for efficiency and scalability.

Java Libraries: Java is not typically used for training massive transformer models from scratch due to performance limitations, but you can:

Implement a small transformer for educational purposes (using libraries like DeepLearning4J).

Integrate with pre-trained models using Java wrappers or JNI (Java Native Interface) to call efficient C++/Python backends.

2. Data Preparation
Dataset: Llama 3.2 is trained on trillions of tokens of multilingual, publicly available data.

For a Java project: Use open datasets like WikiText, OSCAR, or Common Crawl for small-scale experiments.

3. Training (Realistically: Fine-tuning)
Hardware: Training LLMs with billions of parameters requires clusters of GPUs; this is not feasible on typical hardware or with Java alone.

Practical Approach: Fine-tune smaller, open-source models (e.g., 125M–1B parameters) using Python frameworks, then serve them via a Java application.

4. Serving and Integration in Java
Model Serving: Use an inference server (e.g., Hugging Face Transformers, TensorFlow Serving, or ONNX Runtime) to host your model.

Java Integration: Call the inference server from your Java application via REST API or gRPC.

Example: Use Spring Boot to build a REST API that sends prompts to your model server and returns responses.

Example: Java Service for LLM Inference
java
@RestController
@RequestMapping("/api/llm")
public class LLMController {
    @PostMapping("/generate")
    public String generate(@RequestBody String prompt) {
        // Call your inference server (e.g., Python FastAPI, Hugging Face Inference Endpoint)
        // Use HttpClient or RestTemplate to send the prompt and receive the response
        // Return the generated text
    }
}
This approach allows you to leverage powerful LLMs while building your application logic and APIs in Java.

------------------------------------------------------------------------------------------------------------------------------------------------------

What is ollama, OpenAI or Mistral? And why we need them?

Ollama
What it is: Ollama is an open-source tool and framework that lets you run large language models (LLMs) like Llama, Mistral, and others directly on your local machine.

How it works: You download models from the Ollama library and run them locally. This gives you full control over your data and privacy, reduces latency, and removes dependence on cloud services.

Key features:

Local AI model management (download, update, delete models)

Command-line and GUI options for interaction

Multi-platform support (macOS, Linux, Windows)

Ability to integrate custom or fine-tuned models

Contextual conversation management for natural dialogue

Why use it: Ideal for developers, researchers, or businesses that need privacy, offline access, and control over their AI workflows.

OpenAI
What it is: OpenAI is a leading AI research company known for developing advanced LLMs like GPT-3, GPT-4, and DALL-E.

How it works: OpenAI provides access to its models via cloud-based APIs. You send prompts to their servers, and they return responses.

Key features:

State-of-the-art language and vision models

Wide range of applications (chatbots, content generation, code, image synthesis)

Requires internet connection and API key

Why use it: Offers cutting-edge AI capabilities, easy integration via API, and is suitable for production-scale applications where cloud access is acceptable.

Mistral
What it is: Mistral is both a company and a family of open-source LLMs designed for efficiency and strong performance, especially in code generation and large-scale data processing.

How it works: Mistral models can be run locally (e.g., using Ollama) or via cloud APIs, depending on the deployment.

Key features:

Strong at code generation, pattern recognition, and handling large datasets

Open-source, so models can be customized and fine-tuned for specific needs

Can be integrated into local or cloud environments

Why use it: Useful for developers needing code automation, data analysis, and customizable AI solutions with more control than closed cloud models.

Why Do We Need These Tools/Models?
Privacy and Data Control: Tools like Ollama allow you to keep sensitive data on your own hardware, avoiding cloud storage and potential security risks.

Customization: Open-source models (like those in Ollama and Mistral) can be fine-tuned for specific industries, tasks, or languages.

Performance and Independence: Running models locally (Ollama, Mistral) reduces latency and ensures you’re not dependent on external servers or internet connectivity.

Cutting-Edge Capabilities: OpenAI provides access to the latest breakthroughs in AI, making it easy to add advanced features to your applications.

Versatility: These tools support a broad range of use cases—text generation, summarization, translation, code generation, conversational AI, and more.

In summary:
Ollama, OpenAI, and Mistral are platforms or tools that give you access to powerful AI models. You need them to build, test, and deploy AI-driven features in your applications—choosing between them depends on your requirements for privacy, customization, performance, and access to the latest AI technology.

